import os
import shutil
import random
import argparse
from dataclasses import dataclass
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.agents import AgentExecutor, create_react_agent
from langchain.tools.retriever import create_retriever_tool
from env_config import (
    require_llm_api_key,
    get_llm_base_url,
    get_embedding_base_url,
    get_embedding_model,
)

# ======================================================================
# æ¨¡å—ä¸€: é…ç½®ä¸æ•°æ®ç»“æ„
# ======================================================================
_API_KEY = require_llm_api_key()
_LLM_BASE_URL = get_llm_base_url()
_EMBEDDING_BASE_URL = get_embedding_base_url()

@dataclass
class Protocol:
    """å°è£…åè®®ä¿¡æ¯çš„æ•°æ®ç±»ã€‚"""
    name: str
    commands: set

# ======================================================================
# æ¨¡å—äºŒ: æ–‡ä»¶ä¸RAGå·¥å…·
# ======================================================================

def setup_directories(input_dir: str, output_dir: str):
    print(f"--- æ­£åœ¨å‡†å¤‡ç›®å½• '{input_dir}' -> '{output_dir}' ---")
    if not os.path.exists(input_dir):
        raise FileNotFoundError(f"è¾“å…¥ç›®å½• '{input_dir}' ä¸å­˜åœ¨ã€‚")
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)

def read_seed_file(filepath: str) -> str:
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ '{filepath}' æ—¶å‡ºé”™: {e}")
        return None

def save_enriched_seed(output_dir: str, original_filename: str, content: str, variation_index: int):
    base_name, ext = os.path.splitext(original_filename)
    new_filename = f"enriched_{base_name}_{variation_index}{ext}"
    output_path = os.path.join(output_dir, new_filename)
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        print(f"  ä¿å­˜æ–‡ä»¶ '{new_filename}' æ—¶å‡ºé”™: {e}")

def setup_retriever(db_path: str, collection_name: str):
    print(f"--- æ­£åœ¨ä» '{db_path}' (collection: '{collection_name}') åŠ è½½å‘é‡æ•°æ®åº“... ---")
    if not os.path.exists(db_path):
        raise FileNotFoundError(f"ChromaDB ç›®å½•ä¸å­˜åœ¨: {db_path}ã€‚")
    emb_kwargs = {"model": get_embedding_model(), "api_key": _API_KEY}
    if _EMBEDDING_BASE_URL:
        emb_kwargs["base_url"] = _EMBEDDING_BASE_URL
    embeddings = OpenAIEmbeddings(**emb_kwargs)
    vectordb = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings,
        collection_name=collection_name
    )
    return vectordb.as_retriever(search_kwargs={'k': 5})

# ======================================================================
# æ¨¡å—ä¸‰: æ ¸å¿ƒAIæ‰©å……é€»è¾‘ (RAG + ReAct)
# ======================================================================

def analyze_missing_commands(sequence: str, all_commands: set) -> set:
    sequence_lower = sequence.lower()
    present_commands = {cmd for cmd in all_commands if cmd.lower() in sequence_lower}
    return all_commands - present_commands

def enrich_sequence_with_react(sequence: str, protocol: Protocol, retriever, temperature: float, max_enrich_types: int, model: str) -> str:
    """
    ä½¿ç”¨ RAG + ReAct æ™ºèƒ½ä½“ä¸°å¯Œå•æ¡åºåˆ—ã€‚
    """
    missing_commands = analyze_missing_commands(sequence, protocol.commands)
    if not missing_commands:
        print("  åºåˆ—å·²åŒ…å«æ‰€æœ‰æŒ‡å®šå‘½ä»¤ï¼Œæ— éœ€æ‰©å……ã€‚")
        return None

    if max_enrich_types > 0 and len(missing_commands) > max_enrich_types:
        commands_to_add = set(random.sample(list(missing_commands), max_enrich_types))
    else:
        commands_to_add = missing_commands

    # --- RAGæ£€ç´¢æ–‡æ¡£ ---
    print(f"  -> RAG: æ­£åœ¨ä¸ºå‘½ä»¤ {commands_to_add} æ£€ç´¢ä¸Šä¸‹æ–‡...")
    context_str = ""
    for cmd in commands_to_add:
        query = f"How to use the {cmd} command in {protocol.name} protocol, including syntax and examples."
        docs = retriever.invoke(query)
        context_str += f"\n--- Documentation for {cmd} ---\n"
        context_str += "\n".join([doc.page_content for doc in docs])
        context_str += "\n"

    # --- ReActæ™ºèƒ½ä½“ ---
    retriever_tool = create_retriever_tool(
        retriever,
        "search_protocol_docs",
        "Search technical documentation to validate command usage and insertion points."
    )
    tools = [retriever_tool]

    react_prompt = f"""
You are an expert in the {protocol.name} protocol.
You need to intelligently complete an incomplete sequence using authoritative documentation.

Authoritative Context from RAG:
{context_str}

Missing Commands to Insert:
{', '.join(sorted(commands_to_add))}

Rules:
1. Strictly follow the syntax and examples from the context.
2. Only output the raw message sequence.
3. Ensure logical conversation flow for the {protocol.name} protocol.
4. Use ReAct cycle (Thought -> Action -> Observation) to iteratively decide where to insert commands.

Initial Sequence:
{sequence}

Completed Sequence:
"""

    llm_kwargs = {"model": model, "temperature": temperature, "api_key": _API_KEY}
    if _LLM_BASE_URL:
        llm_kwargs["base_url"] = _LLM_BASE_URL
    llm = ChatOpenAI(**llm_kwargs)
    agent = create_react_agent(llm, tools, prompt=react_prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False, handle_parsing_errors=True)

    enriched_sequence = agent_executor.invoke({})['output']
    return enriched_sequence

# ======================================================================
# æ¨¡å—å››: ä¸»æµç¨‹æ§åˆ¶å™¨
# ======================================================================

def main_process(args):
    command_set = {cmd.strip() for cmd in args.commands.split(',') if cmd.strip()}
    protocol = Protocol(name=args.protocol_name, commands=command_set)

    setup_directories(args.input_dir, args.output_dir)
    retriever = setup_retriever(args.db_dir, args.collection_name)
    
    all_seed_files = [f for f in os.listdir(args.input_dir) if os.path.isfile(os.path.join(args.input_dir, f))]
    if not all_seed_files:
        print(f"è­¦å‘Š: è¾“å…¥ç›®å½• '{args.input_dir}' ä¸­æ²¡æœ‰æ‰¾åˆ°ç§å­æ–‡ä»¶ã€‚")
        return

    if args.max_corpus_size > 0 and len(all_seed_files) > args.max_corpus_size:
        seed_files_to_process = random.sample(all_seed_files, args.max_corpus_size)
    else:
        seed_files_to_process = all_seed_files

    print(f"\n--- åè®® '{protocol.name}' | å¼€å§‹æ‰¹é‡æ‰©å…… {len(seed_files_to_process)} ä¸ªç§å­æ–‡ä»¶... ---")
    
    total_generated = 0
    for filename in seed_files_to_process:
        filepath = os.path.join(args.input_dir, filename)
        print(f"\nå¤„ç†åŸå§‹ç§å­: {filename}")
        original_content = read_seed_file(filepath)
        if not original_content: continue
        
        generated_for_this_seed = 0
        for i in range(args.variations):
            temp = random.uniform(0.3, 0.8)
            print(f"  -> ç”Ÿæˆå˜ä½“ {i+1}/{args.variations} (temperature={temp:.2f})...")

            enriched_content = enrich_sequence_with_react(
                original_content, protocol, retriever, temp, args.max_enrich_types, args.model
            )
            
            if enriched_content and enriched_content.strip() != original_content.strip():
                save_enriched_seed(args.output_dir, filename, enriched_content, i + 1)
                generated_for_this_seed += 1
        
        if generated_for_this_seed > 0:
            print(f"  æˆåŠŸä¸º {filename} ç”Ÿæˆäº† {generated_for_this_seed} ä¸ªæ–°ç§å­ã€‚")
        total_generated += generated_for_this_seed

    print(f"\nğŸ‰ æ‰¹é‡æ‰©å……ä»»åŠ¡å®Œæˆï¼æ€»è®¡ç”Ÿæˆäº† {total_generated} ä¸ªæ–°ç§å­ï¼Œå·²ä¿å­˜è‡³ '{args.output_dir}' ç›®å½•ã€‚")

# ======================================================================
# è„šæœ¬å…¥å£
# ======================================================================

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="[RAG+ReActå¢å¼ºç‰ˆ]é€šç”¨åè®®ç§å­ä¸°å¯Œå™¨",
        formatter_class=argparse.RawTextHelpFormatter
    )

    parser.add_argument("--db-dir", default="./chroma_db_protocols", type=str, help="ChromaDBæŒä¹…åŒ–å­˜å‚¨è·¯å¾„ã€‚")
    parser.add_argument("--collection-name", required=True, type=str, help="ChromaDB Collectionåç§°ã€‚")
    parser.add_argument("--protocol-name", required=True, type=str, help="åè®®åç§°ï¼Œä¾‹å¦‚ 'Exim SMTP'ã€‚")
    parser.add_argument("--commands", required=True, type=str, help="åè®®å‘½ä»¤å…¨é›†ï¼Œç”¨é€—å·åˆ†éš”ã€‚")
    parser.add_argument("--input-dir", default="in", type=str, help="è¾“å…¥ç§å­æ–‡ä»¶ç›®å½•ã€‚")
    parser.add_argument("--output-dir", default="out", type=str, help="è¾“å‡ºç›®å½•ã€‚")
    parser.add_argument("--variations", default=5, type=int, help="æ¯ä¸ªç§å­ç”Ÿæˆå˜ä½“æ•°é‡ã€‚")
    parser.add_argument("--model", default="gpt-4-turbo", type=str, help="ä½¿ç”¨çš„OpenAIæ¨¡å‹ã€‚")
    parser.add_argument("--max-enrich-types", default=2, type=int, help="å•æ¬¡ä¸°å¯Œæ“ä½œæœ€å¤šæ·»åŠ å‘½ä»¤ç±»å‹æ•°ã€‚")
    parser.add_argument("--max-corpus-size", default=10, type=int, help="æœ€å¤šå¤„ç†çš„ç§å­æ–‡ä»¶æ•°é‡ã€‚")

    args = parser.parse_args()
    main_process(args)
